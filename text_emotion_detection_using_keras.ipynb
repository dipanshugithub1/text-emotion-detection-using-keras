{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "85c04cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/maximus1/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/maximus1/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/maximus1/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split,KFold, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix, classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e80dfdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/home/maximus1/Downloads/tweet_emotions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e7f707ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1956967666</td>\n",
       "      <td>sadness</td>\n",
       "      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1956967696</td>\n",
       "      <td>sadness</td>\n",
       "      <td>Funeral ceremony...gloomy friday...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1956967789</td>\n",
       "      <td>enthusiasm</td>\n",
       "      <td>wants to hang out with friends SOON!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1956968416</td>\n",
       "      <td>neutral</td>\n",
       "      <td>@dannycastillo We want to trade with someone w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1956968477</td>\n",
       "      <td>worry</td>\n",
       "      <td>Re-pinging @ghostridah14: why didn't you go to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39995</th>\n",
       "      <td>1753918954</td>\n",
       "      <td>neutral</td>\n",
       "      <td>@JohnLloydTaylor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39996</th>\n",
       "      <td>1753919001</td>\n",
       "      <td>love</td>\n",
       "      <td>Happy Mothers Day  All my love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39997</th>\n",
       "      <td>1753919005</td>\n",
       "      <td>love</td>\n",
       "      <td>Happy Mother's Day to all the mommies out ther...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39998</th>\n",
       "      <td>1753919043</td>\n",
       "      <td>happiness</td>\n",
       "      <td>@niariley WASSUP BEAUTIFUL!!! FOLLOW ME!!  PEE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39999</th>\n",
       "      <td>1753919049</td>\n",
       "      <td>love</td>\n",
       "      <td>@mopedronin bullet train from tokyo    the gf ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39173 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         tweet_id   sentiment  \\\n",
       "1      1956967666     sadness   \n",
       "2      1956967696     sadness   \n",
       "3      1956967789  enthusiasm   \n",
       "4      1956968416     neutral   \n",
       "5      1956968477       worry   \n",
       "...           ...         ...   \n",
       "39995  1753918954     neutral   \n",
       "39996  1753919001        love   \n",
       "39997  1753919005        love   \n",
       "39998  1753919043   happiness   \n",
       "39999  1753919049        love   \n",
       "\n",
       "                                                 content  \n",
       "1      Layin n bed with a headache  ughhhh...waitin o...  \n",
       "2                    Funeral ceremony...gloomy friday...  \n",
       "3                   wants to hang out with friends SOON!  \n",
       "4      @dannycastillo We want to trade with someone w...  \n",
       "5      Re-pinging @ghostridah14: why didn't you go to...  \n",
       "...                                                  ...  \n",
       "39995                                   @JohnLloydTaylor  \n",
       "39996                     Happy Mothers Day  All my love  \n",
       "39997  Happy Mother's Day to all the mommies out ther...  \n",
       "39998  @niariley WASSUP BEAUTIFUL!!! FOLLOW ME!!  PEE...  \n",
       "39999  @mopedronin bullet train from tokyo    the gf ...  \n",
       "\n",
       "[39173 rows x 3 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pruned_df = df[df['sentiment']!='empty']\n",
    "pruned_df# removing empty sentiment\n",
    "pruned_df = df[df['sentiment']!='empty']\n",
    "pruned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f0551532",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    text = text.split()\n",
    "    text = [lemmatizer.lemmatize(y) for y in text]\n",
    "    return \" \".join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c7196d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Remove stopwords\n",
    "def remove_stop_words(text):\n",
    "\n",
    "    Text=[i for i in str(text).split() if i not in stop_words]\n",
    "    return \" \".join(Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b5d6d4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. Remove numbers\n",
    "def Removing_numbers(text):\n",
    "    text=''.join([i for i in text if not i.isdigit()])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "654f6776",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. lower case\n",
    "def lower_case(text):\n",
    "    \n",
    "    text = text.split()\n",
    "\n",
    "    text=[y.lower() for y in text]\n",
    "    \n",
    "    return \" \" .join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "39ad22f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Remove punctuations\n",
    "def Removing_punctuations(text):\n",
    "    ## Remove punctuations\n",
    "    text = re.sub('[%s]' % re.escape(\"\"\"!\"#$%&'()*+,،-./:;<=>؟?@[\\]^_`{|}~\"\"\"), ' ', text)\n",
    "    text = text.replace('؛',\"\", )\n",
    "    \n",
    "    ## remove extra whitespace\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    text =  \" \".join(text.split())\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bd4617c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6. Remove urls\n",
    "def Removing_urls(text):\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3cb8aaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(df):\n",
    "    df.content=df.content.apply(lambda text : lower_case(text))\n",
    "    df.content=df.content.apply(lambda text : remove_stop_words(text))\n",
    "    df.content=df.content.apply(lambda text : Removing_numbers(text))\n",
    "    df.content=df.content.apply(lambda text : Removing_punctuations(text))\n",
    "    df.content=df.content.apply(lambda text : Removing_urls(text))\n",
    "    df.content=df.content.apply(lambda text : lemmatization(text))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "34fbb875",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maximus1/Desktop/home/Desktop/envs/myenv/lib/python3.7/site-packages/pandas/core/generic.py:5516: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[name] = value\n"
     ]
    }
   ],
   "source": [
    "pruned_df= normalize_text(pruned_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "34761694",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_content = pruned_df['content']\n",
    "train_sentiment = pruned_df['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "612bde18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting into train and test\n",
    "X_train,X_test,y_train,y_test = train_test_split(train_content,train_sentiment,test_size=0.15,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e4766e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding, Bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "bb7f5ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(y_train)\n",
    "y_test = le.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "cf087169",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0c2f75cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(oov_token='UNK')\n",
    "tokenizer.fit_on_texts(pd.concat([X_train, X_test], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f828422a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_train = tokenizer.texts_to_sequences(X_train)\n",
    "sequences_test = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "582ad846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133\n"
     ]
    }
   ],
   "source": [
    "maxlen = max([len(t) for t in X_train])\n",
    "print(maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5d7bfe7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#padding sequences\n",
    "X_train = pad_sequences(sequences_train, maxlen=maxlen, truncating='pre')\n",
    "X_test = pad_sequences(sequences_test, maxlen=maxlen, truncating='pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "26514f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabSize = len(tokenizer.index_word) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f82267bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[492, 13459, 1284, 69, 8, 13460]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7fce51fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading glove embeddings\n",
    "path_to_glove_file = '/home/maximus1/Downloads/glove.6B/glove.6B.200d.txt'\n",
    "num_tokens = vocabSize\n",
    "embedding_dim = 200 #latent factors or features  \n",
    "hits = 0\n",
    "misses = 0\n",
    "embeddings_index = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a354d07c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400001 word vectors.\n",
      "Converted 20060 words (22673 misses)\n"
     ]
    }
   ],
   "source": [
    "# Read word vectors\n",
    "with open(path_to_glove_file) as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "print(\"Found %s word vectors.\" % len(embeddings_index))\n",
    "\n",
    "# Assign word vectors to our dictionary/vocabulary\n",
    "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        # This includes the representation for \"padding\" and \"OOV\"\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b5591979",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-12 12:17:42.196346: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2024-06-12 12:17:42.196699: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2024-06-12 12:17:42.196731: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (server1.maximus1): /proc/driver/nvidia/version does not exist\n",
      "2024-06-12 12:17:42.203379: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 133, 200)          8546800   \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 133, 1024)        2920448   \n",
      " l)                                                              \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 133, 512)         2623488   \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " bidirectional_2 (Bidirectio  (None, 256)              656384    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense (Dense)               (None, 12)                3084      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,750,204\n",
      "Trainable params: 6,203,404\n",
      "Non-trainable params: 8,546,800\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "adam = Adam(learning_rate=0.01)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocabSize, 200, input_length=X_train.shape[1], weights=[embedding_matrix], trainable=False))\n",
    "model.add(Bidirectional(LSTM(512, dropout=0.2,recurrent_dropout=0.2, return_sequences=True)))\n",
    "model.add(Bidirectional(LSTM(256, dropout=0.2,recurrent_dropout=0.2, return_sequences=True)))\n",
    "model.add(Bidirectional(LSTM(128, dropout=0.2,recurrent_dropout=0.2)))\n",
    "model.add(Dense(12, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "053a948d",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = EarlyStopping(\n",
    "   monitor=\"val_loss\",\n",
    "   patience=4,\n",
    "   restore_best_weights=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc5e8b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    }
   ],
   "source": [
    "# Fit model\n",
    "history = model.fit(X_train,\n",
    "                    y_train,\n",
    "                    verbose=1,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    callbacks=[callback],\n",
    "                    epochs=10,\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5495dec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test, y_test, verbose=1) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
